"""
Pipelineè¯„ä¼°æ¨¡å— - è¯„ä¼°Filterå’ŒEvaluatorçš„å‡†ç¡®æ€§
"""

import json
import random
from typing import Dict, Any, List, Optional
from collections import defaultdict
from datetime import datetime
from google import genai
from google.genai import types

from .base import BaseModule


class PipelineEvaluator(BaseModule):
    """Pipelineè´¨é‡è¯„ä¼°æ¨¡å—"""

    def __init__(self, config_path: str = "config/pipeline_evaluator.yaml"):
        super().__init__(config_path)
        self.sample_size = self.config.get('sample_size', 20)
        self.review_model = self.config.get('review_model', 'gemini-2.5-pro')

        # ä½¿ç”¨æ›´å¼ºçš„æ¨¡å‹è¿›è¡Œå®¡æŸ¥
        self.client = genai.Client(api_key=self.config['gemini_api_key'])

    def _re_evaluate_filter(self, tweet: Dict[str, Any]) -> Dict[str, Any]:
        """
        ç”¨æ›´å¼ºçš„æ¨¡å‹é‡æ–°è¯„ä¼°Filterå†³ç­–

        Returns:
            {
                'should_pass': bool,
                'confidence': int (0-100),
                'reason': str
            }
        """
        content = tweet['content']

        prompt = f"""è¯·ä½œä¸ºä¸€ä¸ªä¸¥æ ¼çš„AIå†…å®¹å®¡æŸ¥ä¸“å®¶ï¼Œåˆ¤æ–­è¿™æ¡æ¨æ–‡æ˜¯å¦ä¸äººå·¥æ™ºèƒ½ç›¸å…³ã€‚

æ¨æ–‡å†…å®¹ï¼š
{content}

è¯„ä¼°æ ‡å‡†ï¼š
- å¿…é¡»æ˜ç¡®è®¨è®º AIã€æœºå™¨å­¦ä¹ ã€å¤§æ¨¡å‹ã€æˆ–ç›¸å…³æŠ€æœ¯
- ä»…ä»…æåˆ°AIäº§å“åç§°ï¼ˆå¦‚ChatGPTï¼‰ä½†ä¸è®¨è®ºæŠ€æœ¯å†…å®¹çš„ï¼Œä¸ç®—ç›¸å…³
- éœ€è¦æœ‰å®è´¨æ€§çš„AIç›¸å…³ä¿¡æ¯ï¼Œä¸æ˜¯ç®€å•æåŠ

è¯·ä»¥JSONæ ¼å¼å›å¤ï¼š
{{
  "should_pass": true/false,
  "confidence": 0-100çš„ç½®ä¿¡åº¦,
  "reason": "åˆ¤æ–­ç†ç”±ï¼ˆ1å¥è¯ï¼‰"
}}"""

        try:
            response = self.client.models.generate_content(
                model=self.review_model,
                contents=prompt,
                config=types.GenerateContentConfig(
                    temperature=0.1,
                    response_mime_type='application/json'
                )
            )
            result = json.loads(response.text.strip())
            return result
        except Exception as e:
            self.logger.error(f"é‡æ–°è¯„ä¼°å¤±è´¥: {e}")
            return {'should_pass': False, 'confidence': 0, 'reason': f'Error: {str(e)}'}

    def _re_evaluate_evaluator(self, tweet: Dict[str, Any]) -> Dict[str, Any]:
        """
        ç”¨æ›´å¼ºçš„æ¨¡å‹é‡æ–°è¯„ä¼°Evaluatorå†³ç­–

        Returns:
            {
                'should_pass': bool,
                'score': int (1-10),
                'reason': str
            }
        """
        content = tweet['content']
        user = tweet['user']

        prompt = f"""è¯·ä½œä¸ºä¸€ä¸ªä¸¥æ ¼çš„å†…å®¹ä»·å€¼è¯„ä¼°ä¸“å®¶ï¼Œè¯„ä¼°è¿™æ¡AIæ¨æ–‡çš„ä»·å€¼ã€‚

ä½œè€…: @{user['username']} (ç²‰ä¸: {user['followers']})
å†…å®¹ï¼š
{content}

äº’åŠ¨æ•°æ®ï¼š
- å›å¤: {tweet['replyCount']}
- è½¬å‘: {tweet['retweetCount']}
- ç‚¹èµ: {tweet['likeCount']}

è¯„åˆ†æ ‡å‡†ï¼ˆ1-10åˆ†ï¼‰ï¼š
- 8-10åˆ†: é«˜ä»·å€¼ï¼ˆåŸåˆ›æ·±åº¦åˆ†æã€é‡è¦æ–°é—»ã€å®ç”¨æŠ€å·§ï¼‰
- 5-7åˆ†: ä¸­ç­‰ä»·å€¼ï¼ˆæœ‰ä¸€å®šä¿¡æ¯é‡ï¼‰
- 1-4åˆ†: ä½ä»·å€¼ï¼ˆçº¯è½¬å‘ã€æ— å®è´¨å†…å®¹ï¼‰

é˜ˆå€¼ï¼š5åˆ†
- â‰¥5åˆ†ï¼šåº”è¯¥é€šè¿‡
- <5åˆ†ï¼šåº”è¯¥æ‹’ç»

è¯·ä»¥JSONæ ¼å¼å›å¤ï¼š
{{
  "should_pass": true/false,
  "score": 1-10çš„è¯„åˆ†,
  "reason": "è¯„åˆ†ç†ç”±ï¼ˆ1å¥è¯ï¼‰"
}}"""

        try:
            response = self.client.models.generate_content(
                model=self.review_model,
                contents=prompt,
                config=types.GenerateContentConfig(
                    temperature=0.1,
                    response_mime_type='application/json'
                )
            )
            result = json.loads(response.text.strip())
            return result
        except Exception as e:
            self.logger.error(f"é‡æ–°è¯„ä¼°å¤±è´¥: {e}")
            return {'should_pass': False, 'score': 0, 'reason': f'Error: {str(e)}'}

    def evaluate_filter_stage(self,
                              filter_rejected_file: str,
                              filter_passed_file: str) -> Dict[str, Any]:
        """
        è¯„ä¼°Filteré˜¶æ®µçš„å‡†ç¡®æ€§

        Args:
            filter_rejected_file: è¢«Filteræ‹’ç»çš„æ¨æ–‡æ–‡ä»¶
            filter_passed_file: é€šè¿‡Filterçš„æ¨æ–‡æ–‡ä»¶

        Returns:
            è¯„ä¼°æŠ¥å‘Š
        """
        self.logger.info("=" * 60)
        self.logger.info("å¼€å§‹è¯„ä¼° Filter é˜¶æ®µ")
        self.logger.info("=" * 60)

        # åŠ è½½æ•°æ®
        rejected_data = self.load_json(filter_rejected_file)
        passed_data = self.load_json(filter_passed_file)

        if not rejected_data or not passed_data:
            self.logger.error("æ— æ³•åŠ è½½æ•°æ®")
            return {}

        rejected_tweets = rejected_data['tweets']
        passed_tweets = passed_data['tweets']

        self.logger.info(f"Filteræ‹’ç»: {len(rejected_tweets)} æ¡")
        self.logger.info(f"Filteré€šè¿‡: {len(passed_tweets)} æ¡")

        # æŠ½æ ·æ£€æŸ¥è¢«æ‹’ç»çš„ï¼ˆæŸ¥æ‰¾å‡é˜´æ€§ï¼‰
        sample_size = min(self.sample_size, len(rejected_tweets))
        rejected_sample = random.sample(rejected_tweets, sample_size)

        self.logger.info(f"\næŠ½æ ·æ£€æŸ¥è¢«æ‹’ç»çš„æ¨æ–‡ï¼ˆæ ·æœ¬é‡: {sample_size}ï¼‰...")

        false_negatives = []
        for i, tweet in enumerate(rejected_sample, 1):
            self.logger.info(f"  å®¡æŸ¥è¿›åº¦: {i}/{sample_size}")
            result = self._re_evaluate_filter(tweet)

            if result['should_pass']:
                false_negatives.append({
                    'tweet_id': tweet['id_str'],
                    'content': tweet['content'][:100] + '...',
                    'original_decision': 'REJECT',
                    'review_decision': 'PASS',
                    'confidence': result['confidence'],
                    'reason': result['reason']
                })

        # æŠ½æ ·æ£€æŸ¥é€šè¿‡çš„ï¼ˆæŸ¥æ‰¾å‡é˜³æ€§ï¼‰
        sample_size_passed = min(self.sample_size, len(passed_tweets))
        passed_sample = random.sample(passed_tweets, sample_size_passed)

        self.logger.info(f"\næŠ½æ ·æ£€æŸ¥é€šè¿‡çš„æ¨æ–‡ï¼ˆæ ·æœ¬é‡: {sample_size_passed}ï¼‰...")

        false_positives = []
        for i, tweet in enumerate(passed_sample, 1):
            self.logger.info(f"  å®¡æŸ¥è¿›åº¦: {i}/{sample_size_passed}")
            result = self._re_evaluate_filter(tweet)

            if not result['should_pass']:
                false_positives.append({
                    'tweet_id': tweet['id_str'],
                    'content': tweet['content'][:100] + '...',
                    'original_decision': 'PASS',
                    'review_decision': 'REJECT',
                    'confidence': result['confidence'],
                    'reason': result['reason']
                })

        # è®¡ç®—å‡†ç¡®ç‡
        false_negative_rate = len(false_negatives) / sample_size if sample_size > 0 else 0
        false_positive_rate = len(false_positives) / sample_size_passed if sample_size_passed > 0 else 0

        report = {
            'stage': 'Filter',
            'total_rejected': len(rejected_tweets),
            'total_passed': len(passed_tweets),
            'pass_rate': len(passed_tweets) / (len(rejected_tweets) + len(passed_tweets)),
            'sample_size_rejected': sample_size,
            'sample_size_passed': sample_size_passed,
            'false_negatives': {
                'count': len(false_negatives),
                'rate': false_negative_rate,
                'examples': false_negatives[:5]  # åªä¿å­˜å‰5ä¸ªä¾‹å­
            },
            'false_positives': {
                'count': len(false_positives),
                'rate': false_positive_rate,
                'examples': false_positives[:5]
            },
            'estimated_accuracy': 1 - (false_negative_rate + false_positive_rate) / 2
        }

        return report

    def evaluate_evaluator_stage(self,
                                  evaluator_rejected_file: str,
                                  evaluator_passed_file: str) -> Dict[str, Any]:
        """
        è¯„ä¼°Evaluatoré˜¶æ®µçš„å‡†ç¡®æ€§

        Args:
            evaluator_rejected_file: è¢«Evaluatoræ‹’ç»çš„æ¨æ–‡æ–‡ä»¶
            evaluator_passed_file: é€šè¿‡Evaluatorçš„æ¨æ–‡æ–‡ä»¶

        Returns:
            è¯„ä¼°æŠ¥å‘Š
        """
        self.logger.info("=" * 60)
        self.logger.info("å¼€å§‹è¯„ä¼° Evaluator é˜¶æ®µ")
        self.logger.info("=" * 60)

        # åŠ è½½æ•°æ®
        rejected_data = self.load_json(evaluator_rejected_file)
        passed_data = self.load_json(evaluator_passed_file)

        if not rejected_data or not passed_data:
            self.logger.error("æ— æ³•åŠ è½½æ•°æ®")
            return {}

        rejected_tweets = rejected_data['tweets']
        passed_tweets = passed_data['tweets']

        self.logger.info(f"Evaluatoræ‹’ç»: {len(rejected_tweets)} æ¡")
        self.logger.info(f"Evaluatoré€šè¿‡: {len(passed_tweets)} æ¡")

        # æ”¶é›†åŸå§‹è¯„åˆ†åˆ†å¸ƒ
        rejected_scores = [t.get('value', {}).get('score', 0) for t in rejected_tweets]
        passed_scores = [t.get('value', {}).get('score', 0) for t in passed_tweets]

        # æŠ½æ ·æ£€æŸ¥è¢«æ‹’ç»çš„
        sample_size = min(self.sample_size, len(rejected_tweets))
        rejected_sample = random.sample(rejected_tweets, sample_size)

        self.logger.info(f"\næŠ½æ ·æ£€æŸ¥è¢«æ‹’ç»çš„æ¨æ–‡ï¼ˆæ ·æœ¬é‡: {sample_size}ï¼‰...")

        false_negatives = []
        score_differences = []

        for i, tweet in enumerate(rejected_sample, 1):
            self.logger.info(f"  å®¡æŸ¥è¿›åº¦: {i}/{sample_size}")
            result = self._re_evaluate_evaluator(tweet)

            original_score = tweet.get('value', {}).get('score', 0)
            review_score = result['score']
            score_diff = review_score - original_score
            score_differences.append(score_diff)

            if result['should_pass']:
                false_negatives.append({
                    'tweet_id': tweet['id_str'],
                    'content': tweet['content'][:100] + '...',
                    'original_score': original_score,
                    'review_score': review_score,
                    'score_diff': score_diff,
                    'reason': result['reason']
                })

        # æŠ½æ ·æ£€æŸ¥é€šè¿‡çš„
        sample_size_passed = min(self.sample_size, len(passed_tweets))
        passed_sample = random.sample(passed_tweets, sample_size_passed)

        self.logger.info(f"\næŠ½æ ·æ£€æŸ¥é€šè¿‡çš„æ¨æ–‡ï¼ˆæ ·æœ¬é‡: {sample_size_passed}ï¼‰...")

        false_positives = []

        for i, tweet in enumerate(passed_sample, 1):
            self.logger.info(f"  å®¡æŸ¥è¿›åº¦: {i}/{sample_size_passed}")
            result = self._re_evaluate_evaluator(tweet)

            original_score = tweet.get('value', {}).get('score', 0)
            review_score = result['score']
            score_diff = review_score - original_score
            score_differences.append(score_diff)

            if not result['should_pass']:
                false_positives.append({
                    'tweet_id': tweet['id_str'],
                    'content': tweet['content'][:100] + '...',
                    'original_score': original_score,
                    'review_score': review_score,
                    'score_diff': score_diff,
                    'reason': result['reason']
                })

        # è®¡ç®—å‡†ç¡®ç‡
        false_negative_rate = len(false_negatives) / sample_size if sample_size > 0 else 0
        false_positive_rate = len(false_positives) / sample_size_passed if sample_size_passed > 0 else 0

        # è®¡ç®—è¯„åˆ†åå·®
        avg_score_diff = sum(score_differences) / len(score_differences) if score_differences else 0

        report = {
            'stage': 'Evaluator',
            'total_rejected': len(rejected_tweets),
            'total_passed': len(passed_tweets),
            'pass_rate': len(passed_tweets) / (len(rejected_tweets) + len(passed_tweets)),
            'sample_size_rejected': sample_size,
            'sample_size_passed': sample_size_passed,
            'false_negatives': {
                'count': len(false_negatives),
                'rate': false_negative_rate,
                'examples': false_negatives[:5]
            },
            'false_positives': {
                'count': len(false_positives),
                'rate': false_positive_rate,
                'examples': false_positives[:5]
            },
            'estimated_accuracy': 1 - (false_negative_rate + false_positive_rate) / 2,
            'score_analysis': {
                'avg_score_diff': avg_score_diff,
                'interpretation': 'Proå®¡æŸ¥æ¨¡å‹å¹³å‡ç»™åˆ†æ›´é«˜' if avg_score_diff > 0 else 'Proå®¡æŸ¥æ¨¡å‹å¹³å‡ç»™åˆ†æ›´ä½'
            }
        }

        return report

    def generate_optimization_suggestions(self,
                                         filter_report: Dict[str, Any],
                                         evaluator_report: Dict[str, Any]) -> List[str]:
        """
        åŸºäºè¯„ä¼°æŠ¥å‘Šç”Ÿæˆä¼˜åŒ–å»ºè®®
        """
        suggestions = []

        # Filteré˜¶æ®µå»ºè®®
        filter_fn_rate = filter_report['false_negatives']['rate']
        filter_fp_rate = filter_report['false_positives']['rate']

        if filter_fn_rate > 0.3:
            suggestions.append(
                f"âš ï¸ Filterå‡é˜´æ€§ç‡è¿‡é«˜({filter_fn_rate:.1%})ï¼šå»ºè®®é™ä½relevance_thresholdé˜ˆå€¼ï¼ˆå½“å‰60ï¼‰æˆ–ä¼˜åŒ–å…³é”®è¯åˆ—è¡¨"
            )

        if filter_fp_rate > 0.3:
            suggestions.append(
                f"âš ï¸ Filterå‡é˜³æ€§ç‡è¿‡é«˜({filter_fp_rate:.1%})ï¼šå»ºè®®æé«˜relevance_thresholdé˜ˆå€¼æˆ–å¢å¼ºLLMåˆ¤æ–­prompt"
            )

        # Evaluatoré˜¶æ®µå»ºè®®
        eval_fn_rate = evaluator_report['false_negatives']['rate']
        eval_fp_rate = evaluator_report['false_positives']['rate']

        if eval_fn_rate > 0.3:
            suggestions.append(
                f"âš ï¸ Evaluatorå‡é˜´æ€§ç‡è¿‡é«˜({eval_fn_rate:.1%})ï¼šå»ºè®®é™ä½value_thresholdé˜ˆå€¼ï¼ˆå½“å‰5ï¼‰"
            )

        if eval_fp_rate > 0.3:
            suggestions.append(
                f"âš ï¸ Evaluatorå‡é˜³æ€§ç‡è¿‡é«˜({eval_fp_rate:.1%})ï¼šå»ºè®®æé«˜value_thresholdé˜ˆå€¼æˆ–ä¼˜åŒ–è¯„ä¼°ç»´åº¦"
            )

        # è¯„åˆ†åå·®å»ºè®®
        score_diff = evaluator_report['score_analysis']['avg_score_diff']
        if abs(score_diff) > 1.5:
            suggestions.append(
                f"âš ï¸ è¯„åˆ†å­˜åœ¨ç³»ç»Ÿæ€§åå·®(å¹³å‡å·®{score_diff:+.1f}åˆ†)ï¼šå»ºè®®è°ƒæ•´Evaluatorçš„temperatureå‚æ•°æˆ–ä¼˜åŒ–prompt"
            )

        # æµç¨‹ä¼˜åŒ–å»ºè®®
        filter_pass_rate = filter_report['pass_rate']
        eval_pass_rate = evaluator_report['pass_rate']
        total_pass_rate = filter_pass_rate * eval_pass_rate

        if filter_pass_rate < 0.2:
            suggestions.append(
                f"ğŸ’¡ Filteré€šè¿‡ç‡è¿‡ä½({filter_pass_rate:.1%})ï¼šè€ƒè™‘æ”¾å®½Filteræ¡ä»¶ï¼Œè®©æ›´å¤šå†…å®¹è¿›å…¥Evaluatoré˜¶æ®µ"
            )

        if filter_pass_rate > 0.8 and eval_pass_rate < 0.3:
            suggestions.append(
                f"ğŸ’¡ Filteré€šè¿‡ç‡é«˜({filter_pass_rate:.1%})ä½†Evaluatoré€šè¿‡ç‡ä½({eval_pass_rate:.1%})ï¼šå»ºè®®åŠ å¼ºFilterç­›é€‰ï¼Œå‡å°‘LLMæˆæœ¬"
            )

        if filter_fn_rate < 0.1 and eval_fn_rate < 0.1:
            suggestions.append(
                f"âœ… ä¸¤é˜¶æ®µå‡†ç¡®ç‡éƒ½å¾ˆé«˜ï¼è€ƒè™‘åˆå¹¶Filterå’ŒEvaluatorä¸ºå•ä¸€LLMè°ƒç”¨ï¼ŒèŠ‚çœ50%æˆæœ¬"
            )

        suggestions.append(
            f"ğŸ“Š æ•´ä½“æ•°æ®ï¼šæœ€ç»ˆé€šè¿‡ç‡ {total_pass_rate:.1%}ï¼ˆFilter {filter_pass_rate:.1%} Ã— Evaluator {eval_pass_rate:.1%}ï¼‰"
        )

        return suggestions

    def run(self, raw_data_file: str) -> str:
        """
        è¿è¡Œå®Œæ•´çš„Pipelineè¯„ä¼°

        Args:
            raw_data_file: åŸå§‹æ•°æ®æ–‡ä»¶è·¯å¾„

        Returns:
            è¯„ä¼°æŠ¥å‘Šæ–‡ä»¶è·¯å¾„
        """
        from modules.filter import Filter
        from modules.evaluator import Evaluator

        self.logger.info("=" * 60)
        self.logger.info("Pipeline è¯„ä¼°å¼€å§‹")
        self.logger.info("=" * 60)

        # ç¬¬ä¸€æ­¥ï¼šè¿è¡ŒFilter
        self.logger.info("\nç¬¬ä¸€æ­¥ï¼šè¿è¡Œ Filter æ¨¡å—...")
        filter_module = Filter()
        filtered_file = filter_module.run(raw_data_file)

        if not filtered_file:
            self.logger.error("Filteré˜¶æ®µæ— è¾“å‡ºï¼Œè¯„ä¼°ç»ˆæ­¢")
            return None

        # ç¬¬äºŒæ­¥ï¼šè¿è¡ŒEvaluator
        self.logger.info("\nç¬¬äºŒæ­¥ï¼šè¿è¡Œ Evaluator æ¨¡å—...")
        evaluator_module = Evaluator()
        evaluated_file = evaluator_module.run(filtered_file)

        if not evaluated_file:
            self.logger.error("Evaluatoré˜¶æ®µæ— è¾“å‡ºï¼Œè¯„ä¼°ç»ˆæ­¢")
            return None

        # ç¬¬ä¸‰æ­¥ï¼šè¯„ä¼°Filter
        filter_rejected_file = raw_data_file.replace('/raw/', '/rejected/filter_')
        filter_report = self.evaluate_filter_stage(filter_rejected_file, filtered_file)

        # ç¬¬å››æ­¥ï¼šè¯„ä¼°Evaluator
        evaluator_rejected_file = filtered_file.replace('/filtered/', '/rejected/evaluator_')
        evaluator_report = self.evaluate_evaluator_stage(evaluator_rejected_file, evaluated_file)

        # ç¬¬äº”æ­¥ï¼šç”Ÿæˆä¼˜åŒ–å»ºè®®
        self.logger.info("=" * 60)
        self.logger.info("ç”Ÿæˆä¼˜åŒ–å»ºè®®")
        self.logger.info("=" * 60)
        suggestions = self.generate_optimization_suggestions(filter_report, evaluator_report)

        # ä¿å­˜å®Œæ•´æŠ¥å‘Š
        report = {
            'evaluation_time': datetime.now().isoformat(),
            'raw_data_file': raw_data_file,
            'filter_report': filter_report,
            'evaluator_report': evaluator_report,
            'optimization_suggestions': suggestions
        }

        output_file = raw_data_file.replace('/raw/', '/analysis/evaluation_')
        self.save_json(report, output_file)

        # æ‰“å°æŠ¥å‘Š
        self.logger.info("\n" + "=" * 60)
        self.logger.info("ğŸ“Š è¯„ä¼°æŠ¥å‘Šæ‘˜è¦")
        self.logger.info("=" * 60)

        self.logger.info(f"\nã€Filter é˜¶æ®µã€‘")
        self.logger.info(f"  æ€»è®¡: {filter_report['total_rejected'] + filter_report['total_passed']} æ¡")
        self.logger.info(f"  é€šè¿‡: {filter_report['total_passed']} æ¡ ({filter_report['pass_rate']:.1%})")
        self.logger.info(f"  æ‹’ç»: {filter_report['total_rejected']} æ¡")
        self.logger.info(f"  å‡é˜´æ€§ç‡: {filter_report['false_negatives']['rate']:.1%} ({filter_report['false_negatives']['count']}/{filter_report['sample_size_rejected']} æ ·æœ¬)")
        self.logger.info(f"  å‡é˜³æ€§ç‡: {filter_report['false_positives']['rate']:.1%} ({filter_report['false_positives']['count']}/{filter_report['sample_size_passed']} æ ·æœ¬)")
        self.logger.info(f"  ä¼°è®¡å‡†ç¡®ç‡: {filter_report['estimated_accuracy']:.1%}")

        self.logger.info(f"\nã€Evaluator é˜¶æ®µã€‘")
        self.logger.info(f"  æ€»è®¡: {evaluator_report['total_rejected'] + evaluator_report['total_passed']} æ¡")
        self.logger.info(f"  é€šè¿‡: {evaluator_report['total_passed']} æ¡ ({evaluator_report['pass_rate']:.1%})")
        self.logger.info(f"  æ‹’ç»: {evaluator_report['total_rejected']} æ¡")
        self.logger.info(f"  å‡é˜´æ€§ç‡: {evaluator_report['false_negatives']['rate']:.1%} ({evaluator_report['false_negatives']['count']}/{evaluator_report['sample_size_rejected']} æ ·æœ¬)")
        self.logger.info(f"  å‡é˜³æ€§ç‡: {evaluator_report['false_positives']['rate']:.1%} ({evaluator_report['false_positives']['count']}/{evaluator_report['sample_size_passed']} æ ·æœ¬)")
        self.logger.info(f"  ä¼°è®¡å‡†ç¡®ç‡: {evaluator_report['estimated_accuracy']:.1%}")
        self.logger.info(f"  è¯„åˆ†åå·®: {evaluator_report['score_analysis']['interpretation']}")

        self.logger.info(f"\nã€ä¼˜åŒ–å»ºè®®ã€‘")
        for suggestion in suggestions:
            self.logger.info(f"  {suggestion}")

        self.logger.info(f"\nå®Œæ•´æŠ¥å‘Šå·²ä¿å­˜: {output_file}")

        return output_file
